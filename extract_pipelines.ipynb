{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcc290f-b577-4ebd-8aa9-d45b21c94100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-mgmt-datafactory\r\n",
      "  Using cached azure_mgmt_datafactory-6.1.0-py3-none-any.whl (525 kB)\r\n",
      "Collecting isodate>=0.6.1\r\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\r\n",
      "Collecting azure-common>=1.1\r\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\r\n",
      "Collecting azure-mgmt-core>=1.3.2\r\n",
      "  Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting azure-core<2.0.0,>=1.26.2\r\n",
      "  Using cached azure_core-1.30.1-py3-none-any.whl (193 kB)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (1.16.0)\r\n",
      "Collecting typing-extensions>=4.6.0\r\n",
      "  Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2.27.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.26.2->azure-mgmt-core>=1.3.2->azure-mgmt-datafactory) (2021.10.8)\r\n",
      "Installing collected packages: typing-extensions, azure-core, isodate, azure-mgmt-core, azure-common, azure-mgmt-datafactory\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing-extensions 4.1.1\r\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-7e9cad02-859e-4e6a-bcf0-352ba9c35676\r\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\r\n",
      "Successfully installed azure-common-1.1.28 azure-core-1.30.1 azure-mgmt-core-1.4.0 azure-mgmt-datafactory-6.1.0 isodate-0.6.1 typing-extensions-4.11.0\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-7e9cad02-859e-4e6a-bcf0-352ba9c35676/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-mgmt-datafactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36d91842-9246-4146-98d8-be989e272850",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-identity\r\n",
      "  Using cached azure_identity-1.16.0-py3-none-any.whl (166 kB)\r\n",
      "Collecting msal>=1.24.0\r\n",
      "  Using cached msal-1.28.0-py3-none-any.whl (102 kB)\r\n",
      "Requirement already satisfied: cryptography>=2.5 in /databricks/python3/lib/python3.9/site-packages (from azure-identity) (3.4.8)\r\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7e9cad02-859e-4e6a-bcf0-352ba9c35676/lib/python3.9/site-packages (from azure-identity) (1.30.1)\r\n",
      "Collecting msal-extensions>=0.3.0\r\n",
      "  Using cached msal_extensions-1.1.0-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.23.0->azure-identity) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7e9cad02-859e-4e6a-bcf0-352ba9c35676/lib/python3.9/site-packages (from azure-core>=1.23.0->azure-identity) (4.11.0)\r\n",
      "Requirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core>=1.23.0->azure-identity) (2.27.1)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.9/site-packages (from cryptography>=2.5->azure-identity) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\r\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\r\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\r\n",
      "Collecting portalocker<3,>=1.0\r\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\r\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from msal-extensions>=0.3.0->azure-identity) (21.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-identity) (2021.10.8)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->msal-extensions>=0.3.0->azure-identity) (3.0.4)\r\n",
      "Installing collected packages: PyJWT, portalocker, msal, msal-extensions, azure-identity\r\n",
      "Successfully installed PyJWT-2.8.0 azure-identity-1.16.0 msal-1.28.0 msal-extensions-1.1.0 portalocker-2.8.2\r\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\r\n",
      "You should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-7e9cad02-859e-4e6a-bcf0-352ba9c35676/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f99451-a332-4eb3-bd60-dc4fa9630f2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, ClientSecretCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623efc52-c89a-4db4-ac7e-07c4073cfd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configurar credenciais\n",
    "credentials = ClientSecretCredential(client_id='', client_secret='', tenant_id='') \n",
    "\n",
    "# Substitua os valores abaixo com suas informações\n",
    "subscription_id = ''\n",
    "resource_group_name = ''\n",
    "data_factory_name = ''\n",
    "\n",
    "# Inicializar o cliente do Azure Data Factory\n",
    "adf_client = DataFactoryManagementClient(credentials, subscription_id)\n",
    "pipelines = adf_client.pipelines.list_by_factory(resource_group_name, data_factory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56469ca3-e5dd-431e-a462-5664eae47be8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipe_id = 0\n",
    "dados_piplines = []\n",
    "dados_inputs = []\n",
    "dados_outputs = []\n",
    "for pipeline in pipelines:\n",
    "    pipe_id += 1\n",
    "    nome_pipeline = pipeline.name\n",
    "    descricao = pipeline.description\n",
    "\n",
    "    dados_dict = {\n",
    "        \"ID\": pipe_id,\n",
    "        \"NOME\": nome_pipeline,\n",
    "        \"DESCRICAO\": descricao}\n",
    "    dados_piplines.append(dados_dict)\n",
    "\n",
    "    pipeline_dict = pipeline.serialize()\n",
    "\n",
    "    response = adf_client.pipelines.get(resource_group_name, data_factory_name, pipeline_name=f\"{nome_pipeline}\")\n",
    "\n",
    "    activities = response.serialize()\n",
    "    for activity in activities[\"properties\"][\"activities\"]:\n",
    "        act_name = activity['name']\n",
    "        act_inputs = activity.get('inputs', [])\n",
    "        act_outputs = activity.get('outputs', [])\n",
    "        query = activity.get('typeProperties', {}).get('source', {}).get('sqlReaderQuery', '')\n",
    "\n",
    "        active_info_origem = {\n",
    "            \"ID\": pipe_id,\n",
    "            \"NOME\": act_name,\n",
    "            \"TIPO\" : act_inputs[0][\"type\"] if act_inputs else '',\n",
    "            \"NOME_REFERENCIA\" : act_inputs[0][\"referenceName\"] if act_inputs else '',\n",
    "            \"PARAMETROS\" : act_inputs[0][\"parameters\"] if act_inputs else None,\n",
    "            \"QUERY\": query\n",
    "        }\n",
    "        active_info_coletor = {\n",
    "            \"ID\": pipe_id,\n",
    "            \"NOME\": act_name,\n",
    "            \"TIPO\" : act_outputs[0][\"type\"] if act_outputs else '',\n",
    "            \"NOME_REFERENCIA\" : act_outputs[0][\"referenceName\"] if act_outputs else '',\n",
    "            \"PARAMETROS\" : act_outputs[0][\"parameters\"] if act_outputs else ''\n",
    "        }\n",
    "        \n",
    "        dados_inputs.append(active_info_origem)\n",
    "        dados_outputs.append(active_info_coletor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8339215-92e5-4820-8007-7c18e55717db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dados_inputs_json = json.dumps(dados_inputs)\n",
    "dados_outputs_json = json.dumps(dados_outputs)\n",
    "dados_piplines_json = json.dumps(dados_piplines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9db45d0-a5bd-4af9-8843-18f4df1a18aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>NOME</th><th>DESCRICAO</th></tr></thead><tbody><tr><td>1</td><td>PL_copy_csv_db_cadastros</td><td>Cópia do csv -datalake para o banco sql</td></tr><tr><td>2</td><td>PL_copy_csv_produtos_vendas</td><td>null</td></tr><tr><td>3</td><td>copia_tabela_schema_dbo</td><td>Copia tabela de vendas do schema empresa para o schema dbo</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "PL_copy_csv_db_cadastros",
         "Cópia do csv -datalake para o banco sql"
        ],
        [
         2,
         "PL_copy_csv_produtos_vendas",
         null
        ],
        [
         3,
         "copia_tabela_schema_dbo",
         "Copia tabela de vendas do schema empresa para o schema dbo"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NOME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DESCRICAO",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_json(dados_piplines_json)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b96d823-55d7-4b0b-bfca-dcad672061cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>NOME</th><th>TIPO</th><th>NOME_REFERENCIA</th><th>PARAMETROS</th><th>QUERY</th></tr></thead><tbody><tr><td>1</td><td>copia csv cliente</td><td>DatasetReference</td><td>DL_dinamico_virgula</td><td>Map(file_name -> CadastroClientes.csv)</td><td></td></tr><tr><td>1</td><td>copia csv - Lojas</td><td>DatasetReference</td><td>DL_dinamico_virgula</td><td>Map(file_name -> Contoso - Lojas.csv)</td><td></td></tr><tr><td>1</td><td>copia csv -funcionarios</td><td>DatasetReference</td><td>DL_dinamico_ponto_virgula</td><td>Map(file_name -> CadastroFuncionarios.csv)</td><td></td></tr><tr><td>2</td><td>copia csv produtos</td><td>DatasetReference</td><td>DL_dinamico_ponto_virgula</td><td>Map(file_name -> Contoso - Cadastro Produtos.csv)</td><td></td></tr><tr><td>2</td><td>inner vendas</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> vendas, schema_name -> empresa)</td><td>SELECT\n",
       "v.[Numero da Venda], \n",
       "v.[Data da Venda], \n",
       "v.[Quantidade Vendida], \n",
       "v.[Quantidade Devolvida],\n",
       "p.[Nome da Marca],\n",
       "p.Tipo,\n",
       "p.Categoria,\n",
       "p.[Preco Unitario],\n",
       "l.[Nome da Loja] \n",
       "FROM empresa.vendas v \n",
       "LEFT JOIN empresa.lojas l  ON l.[ID Loja] =v.[ID Loja]  \n",
       "LEFT JOIN empresa.produtos p ON p.[ID Produto] = v.[ID Produto] </td></tr><tr><td>3</td><td>copy_tabela_vendas_schema_dbo</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> vendas, schema_name -> empresa)</td><td>SELECT\n",
       "v.[Numero da Venda], \n",
       "v.[Data da Venda], \n",
       "v.[Quantidade Vendida], \n",
       "v.[Quantidade Devolvida],\n",
       "p.[Nome da Marca],\n",
       "p.Tipo,\n",
       "p.Categoria,\n",
       "p.[Preco Unitario],\n",
       "l.[Nome da Loja] \n",
       "FROM empresa.vendas v \n",
       "LEFT JOIN empresa.lojas l  ON l.[ID Loja] =v.[ID Loja]  \n",
       "LEFT JOIN empresa.produtos p ON p.[ID Produto] = v.[ID Produto] </td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "copia csv cliente",
         "DatasetReference",
         "DL_dinamico_virgula",
         {
          "file_name": "CadastroClientes.csv"
         },
         ""
        ],
        [
         1,
         "copia csv - Lojas",
         "DatasetReference",
         "DL_dinamico_virgula",
         {
          "file_name": "Contoso - Lojas.csv"
         },
         ""
        ],
        [
         1,
         "copia csv -funcionarios",
         "DatasetReference",
         "DL_dinamico_ponto_virgula",
         {
          "file_name": "CadastroFuncionarios.csv"
         },
         ""
        ],
        [
         2,
         "copia csv produtos",
         "DatasetReference",
         "DL_dinamico_ponto_virgula",
         {
          "file_name": "Contoso - Cadastro Produtos.csv"
         },
         ""
        ],
        [
         2,
         "inner vendas",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "vendas"
         },
         "SELECT\nv.[Numero da Venda], \nv.[Data da Venda], \nv.[Quantidade Vendida], \nv.[Quantidade Devolvida],\np.[Nome da Marca],\np.Tipo,\np.Categoria,\np.[Preco Unitario],\nl.[Nome da Loja] \nFROM empresa.vendas v \nLEFT JOIN empresa.lojas l  ON l.[ID Loja] =v.[ID Loja]  \nLEFT JOIN empresa.produtos p ON p.[ID Produto] = v.[ID Produto] "
        ],
        [
         3,
         "copy_tabela_vendas_schema_dbo",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "vendas"
         },
         "SELECT\nv.[Numero da Venda], \nv.[Data da Venda], \nv.[Quantidade Vendida], \nv.[Quantidade Devolvida],\np.[Nome da Marca],\np.Tipo,\np.Categoria,\np.[Preco Unitario],\nl.[Nome da Loja] \nFROM empresa.vendas v \nLEFT JOIN empresa.lojas l  ON l.[ID Loja] =v.[ID Loja]  \nLEFT JOIN empresa.produtos p ON p.[ID Produto] = v.[ID Produto] "
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NOME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TIPO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "NOME_REFERENCIA",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PARAMETROS",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "QUERY",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_inp = pd.read_json(dados_inputs_json)\n",
    "display(df_inp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c93a5a1-891d-4b88-a2a8-d9d26b954bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/pandas/conversion.py:467: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  A field of type StructType expects a pandas.DataFrame, but got: <class 'pandas.core.series.Series'>\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ID</th><th>NOME</th><th>TIPO</th><th>NOME_REFERENCIA</th><th>PARAMETROS</th></tr></thead><tbody><tr><td>1</td><td>copia csv cliente</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> clientes, schema_name -> empresa)</td></tr><tr><td>1</td><td>copia csv - Lojas</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> lojas, schema_name -> empresa)</td></tr><tr><td>1</td><td>copia csv -funcionarios</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> funcionarios, schema_name -> empresa)</td></tr><tr><td>2</td><td>copia csv produtos</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> produtos, schema_name -> empresa)</td></tr><tr><td>2</td><td>inner vendas</td><td>DatasetReference</td><td>sql_dinamico</td><td>Map(table_name -> juncao_vendas, schema_name -> empresa)</td></tr><tr><td>3</td><td>copy_tabela_vendas_schema_dbo</td><td>DatasetReference</td><td>AzureSqlTable_dbo_clientes</td><td>Map()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "copia csv cliente",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "clientes"
         }
        ],
        [
         1,
         "copia csv - Lojas",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "lojas"
         }
        ],
        [
         1,
         "copia csv -funcionarios",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "funcionarios"
         }
        ],
        [
         2,
         "copia csv produtos",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "produtos"
         }
        ],
        [
         2,
         "inner vendas",
         "DatasetReference",
         "sql_dinamico",
         {
          "schema_name": "empresa",
          "table_name": "juncao_vendas"
         }
        ],
        [
         3,
         "copy_tabela_vendas_schema_dbo",
         "DatasetReference",
         "AzureSqlTable_dbo_clientes",
         {}
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "NOME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TIPO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "NOME_REFERENCIA",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PARAMETROS",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_out = pd.read_json(dados_outputs_json)\n",
    "display(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "653ec5f9-36e0-4046-8d0c-d442cb1322f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
